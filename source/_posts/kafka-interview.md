---
title: kafka-interview
date: 2023-03-26 13:43:54
tags:
- Kafka
- 面试
categories:
- 技术
---
&emsp;&emsp;&emsp;这是一篇关于Kafka面试题的文章，主要是记录本人在面试过程中常见的一些题目以及自己认为可能会考察的面试题。当然里面的内容也是用我自己的话术来总结的，可能存在不严谨的地方，还望指出！！！
## Kafka
构成：
* 生产者：向broker发送消息的客户端。
* 消费者：从broker获取消息的客户端
* 消费者组：由多个消费者组成，消费者组内每个消费者负责不同分区的数据，一个分区只能由一个组内消费者消费，未声明消费者组的消费者所有的消费者都属于一个组。
* Borker：一台kafka服务器就是一个broker，一个集群由多个broker组成，一个broker可以容纳多个topic。
* Topic：可以理解为一个队列，生产者和消费者面向的都是一个topic。
* Partation分区：一个topic可以分为多个partition，每个partition是一个有序队列。
* Replica副本，一个topic的每个分区都有若干个副本（1leader和多个follower）
* Leader：生产者发送数据以及消费者消费数据都是leader。
* follower：主要负责实时同步数据，以备不时之需。

**为什么说kafka快**
采用分区来提高并行度
高效的文件数据结构，采用稀疏索引，在读取数据的时候可以快速定位。
顺序读写磁盘

**分区的好处：**1.可以合理的存储，实现负载均衡。2.可以提高并行度

**副本的作用** 提高数据的可靠性，主要也是备份的作用。

**ACK的设置与作用** 
0：表示不需要收到leader的ack的应答就发送下一个(高吞吐，低一致性)
1：表示只要收到leader应答就发下一个（默认）
-1：表示要收到leader和isr的应答才发送下一个

**数据重复的情况：** 当ack应答设置为-1时，生产者发来消息，leader和isr队列所有节点都同步完数据之后，leader在做应答的时候挂了，此时需要重新选leader，选完之后生产者又发送了一次这个消息，就会发生重复。

*解决：* 我们可以通过kafka的幂等性来解决这个问题，kafka通过三个id保证消息的不重复，分别是生产者ID，还有一个递增的序列号。保证这两个个不同时相同即可判断消息是否重复。
ProducerID：在每个新的Producer初始化时，会被分配一个唯一的ProducerID，这个ProducerID对客户端使用者是不可见的。
SequenceNumber：对于每个ProducerID，Producer发送数据的每个Topic和Partition都对应一个从0开始单调递增的SequenceNumber值。
参考地址：https://juejin.cn/post/7172897190627508237

**数据有序是如何保证的** 
kafka是保证单分区内数据是有序的，全局中是没办法保证有序的，然后单分区有序是通过消息的seqNumber来实现的，如果需要做到全局有序的话，那就是设置一个topic对应一个分区就好，然后消费者也可以设置成单线程的，这样的话就可以避免消费端乱序了。

**kafka的选举机制**
满足在ISR中存活，然后按照副本集合的排列顺序，依次选举。
ISR：保持和leader同步的follower集合，（in sync replica）
OSR：落后于leader的副本集合 （out sync replica）

**leader故障后处理细节** 
当leader阶段发生故障之后，从ISR中重新选取一个leader之后，为了保证多个副本之间的一致，会把所有follower的log文件中高于高水位线的部分都截取掉，然后从新的leader中再去同步。
高水位线：hw（high water）是所有副本中最小的offset位置。

**文件存储结构**
kafka发送数据之后，kafka是如何进行存储的？
kafka中的topic是逻辑上的概念，而partation是物理上的概念，实际上的数据存储是每个partition对应1个log文件夹，然后这个log文件夹存储的就是生产者发送的数据，数据会以追加的形式不断添加，同时，为了防止log文件过大，采用分段的方式，将partition分为多个segment段，每个segment对应有log，index，timeindex文件，这样可以通过小文件快速的定期删除或者删除已经消费完的数据。
**如何通过offset查找数据：**
首先，index和log文件的文件名是以当前segment的第一条消息的offset命名的，然后查找的时候就可以通过二分查找的方式来找到当前index文件和log文件。
然后，通过index文件找到对应的数据在log文件中的大致的位置，向下遍历直到找到对应的记录
**消费者与分区的分配方式：**
range：就是均匀分配，每个消费者去消费几个分区
roundRobin：轮询分配，依次分配
sticky：粘性分区，

**offset的作用**
用于标示当前分区中的偏移量，消费到哪里了。然后维护offset有两种方式：1.自动提交 2.手动提交

**漏消费的情况**
先提交offset，后消费可能发生漏消费
**重复消费**
已经消费了，但是offset没有提交
**解决办法** 我们将消费消息和提交offset的过程做原子绑定。保证全部执行或者全部不执行。

**消息堆积**
1。说明消费能力不行：可以增加消费者的数量，同时我们也可以增加topic的分区的数量。让消费者数和分区数一致
2。我们可能每批次拉取的数据少，然后导致拉取数据的速度小于生产数据的速度，导致积压，我们可以提高每批次拉取数据的量。